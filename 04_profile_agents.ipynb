{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d3fadff9-f023-49bb-be6e-9f4ed2295ba8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Profile Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5f9ed78-e7b8-421b-a0f1-755b4e17c10f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %pip install -U -qqqq mlflow langchain langgraph databricks-langchain pydantic databricks-agents \n",
    "# dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ffcb8e7-ec1d-4f03-82da-6eeb1560b656",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "from databricks_langchain import VectorSearchRetrieverTool\n",
    "from databricks_langchain import DatabricksEmbeddings\n",
    "from unitycatalog.ai.core.databricks import DatabricksFunctionClient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6f7576c9-580c-43bc-9384-9550624a89ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 1: Define the Agent in Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a583b2aa-e1b1-4507-a6af-7dd6beef9e1b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%%writefile agent.py\n",
    "from typing import Any, Generator, Optional, Sequence, Union\n",
    "\n",
    "import json\n",
    "import os\n",
    "import mlflow\n",
    "from databricks_langchain import ChatDatabricks, VectorSearchRetrieverTool\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.language_models import LanguageModelLike\n",
    "from langchain_core.runnables import RunnableConfig, RunnableLambda\n",
    "from langchain_core.tools import BaseTool\n",
    "from langgraph.graph import END, StateGraph\n",
    "from langgraph.graph.graph import CompiledGraph\n",
    "from langgraph.graph.state import CompiledStateGraph\n",
    "from langchain.prompts import PromptTemplate\n",
    "from mlflow.langchain.chat_agent_langgraph import ChatAgentState, ChatAgentToolNode\n",
    "from mlflow.pyfunc import ChatAgent\n",
    "from mlflow.types.agent import (\n",
    "    ChatAgentChunk,\n",
    "    ChatAgentMessage,\n",
    "    ChatAgentResponse,\n",
    "    ChatContext,\n",
    ")\n",
    "\n",
    "mlflow.langchain.autolog()\n",
    "\n",
    "\n",
    "############################################\n",
    "# Define your LLM endpoint and system prompt\n",
    "############################################\n",
    "LLM_ENDPOINT_NAME = \"databricks-meta-llama-3-3-70b-instruct\"\n",
    "VS_INDEX_NAME = \"jack_sandom.ai_audience_segments.ad_campaigns_index\" #@TODO REPLACE WITH YOUR INDEX\n",
    "llm = ChatDatabricks(endpoint=LLM_ENDPOINT_NAME)\n",
    "\n",
    "system_prompt = PromptTemplate(\n",
    "    input_variables=[\"segment\", \"profile\", \"retrieved_ads\"],\n",
    "    template=\"\"\"\n",
    "    You are an audience persona named {segment} with the following profile:\n",
    "    {profile}\n",
    "\n",
    "    The user is an advertising content writer and wants to tailor copy specific to your persona. Your goal is to assist the user in doing this by acting as a {segment} and helping the user to test ideas and get to tailored ad content which is effective on your persona.\n",
    "\n",
    "    {retrieved_ads}\n",
    "\n",
    "    If prompted to improve or generate new ad content, always provide suggested copy. Always end by asking a question or offering a suggestion to help the user get to their goal.\n",
    "\n",
    "    Stay in character always and respond to questions as this persona but be concise where possible. Only respond in the context of your audience persona but don't refer to yourself by the segment name. Keep the information about your persona from the profile provided only and do not give yourself a gender, nationality, ethnicity or sexuality. Do not make stuff up. If asked about something unrelated, politely redirect the conversation.\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "#####################################\n",
    "# Define Vector Search Retriever tool\n",
    "#####################################\n",
    "vs_tool = VectorSearchRetrieverTool(\n",
    "  index_name=VS_INDEX_NAME,\n",
    "  num_results=1,\n",
    "  columns=[\"campaign_id\", \"ad_copy\"],\n",
    "  tool_name=\"Ad-Copy-Retriever\",\n",
    "  tool_description=\"Retrieve prior successful ad copy for segment\",\n",
    "  filters={\"segment\": None}, # Placeholder for dynamic filtering\n",
    ")\n",
    "\n",
    "#####################\n",
    "## Define agent logic\n",
    "#####################\n",
    "\n",
    "def create_profile_agent(\n",
    "    model: LanguageModelLike\n",
    ") -> CompiledGraph:\n",
    "\n",
    "    def generate_prompt_with_profile(state: ChatAgentState):\n",
    "        \"\"\"\n",
    "        Retrieves the customer profile and formats the system prompt dynamically,\n",
    "        including relevant ad copy retrieved using vector search if applicable.\n",
    "        \"\"\"\n",
    "        custom_inputs = state.get(\"custom_inputs\", {})\n",
    "        segment = custom_inputs.get(\"segment\", \"Casual Users\")\n",
    "        \n",
    "        profile = state[\"context\"].get(\n",
    "            \"profile\", \"A casual user doesn't think too much about the product. They will just buy whatever is convenient or cheapest.\"\n",
    "        )\n",
    "        \n",
    "        retrieved_ads = \"\"\n",
    "        \n",
    "        # Let the model decide whether to invoke the tool\n",
    "        tool_decision_prompt = f\"\"\"\n",
    "        You are an AI assistant that decides whether retrieving past ad copy is useful.\n",
    "        \n",
    "        User query: \"{state[\"messages\"][-1][\"content\"]}\"\n",
    "        \n",
    "        Instructions:\n",
    "        - If the user is asking about improving ad copy or writing an ad, return ONLY 'yes'.\n",
    "        - Otherwise, return ONLY 'no'.\n",
    "        \"\"\"\n",
    "        \n",
    "        decision = llm.invoke(tool_decision_prompt).content.strip().lower()\n",
    "        \n",
    "        if decision == \"yes\":\n",
    "            vs_tool.filters = {\"segment\": segment}\n",
    "            tool_response = vs_tool.invoke(state[\"messages\"][-1][\"content\"])\n",
    "            if tool_response:\n",
    "                retrieved_ads = \"\".join([f\"{doc.page_content}\" for doc in tool_response])\n",
    "        \n",
    "        retrieved_ads_text = f\"\"\"Here is a past successful ad for this segment:\n",
    "        {retrieved_ads}\n",
    "        \n",
    "        Use this ad as inspiration if it is relevant to the user's query. If it is not relevant, ignore.\"\"\" if retrieved_ads else \"\"\n",
    "\n",
    "        formatted_prompt = system_prompt.format(\n",
    "            segment=segment,\n",
    "            profile=profile,\n",
    "            retrieved_ads=retrieved_ads_text\n",
    "        )\n",
    "\n",
    "        return [{\"role\": \"system\", \"content\": formatted_prompt}] + state[\"messages\"]\n",
    "\n",
    "    model_runnable = RunnableLambda(generate_prompt_with_profile) | model\n",
    "\n",
    "    def call_model(state: ChatAgentState, config: RunnableConfig):\n",
    "        \"\"\"Calls the model to generate responses using the formatted system prompt.\"\"\"\n",
    "        response = model_runnable.invoke(state, config)\n",
    "        return {\"messages\": [response]}\n",
    "\n",
    "    workflow = StateGraph(ChatAgentState)\n",
    "    workflow.add_node(\"agent\", RunnableLambda(call_model))\n",
    "    workflow.set_entry_point(\"agent\")\n",
    "\n",
    "    return workflow.compile()\n",
    "\n",
    "\n",
    "class LangGraphChatAgent(ChatAgent):\n",
    "    def __init__(self, agent: CompiledStateGraph, profiles_path: str = None):\n",
    "        self.agent = agent\n",
    "        self.PROFILES = {}\n",
    "\n",
    "    def load_context(self, context):\n",
    "        \"\"\"\n",
    "        Loads customer profiles from MLflow artifacts when the model is served.\n",
    "        \"\"\"\n",
    "        config_path = context.artifacts.get(\"profiles\")\n",
    "        json_path = os.path.join(config_path, \"profiles.json\")\n",
    "\n",
    "        if not os.path.exists(json_path):\n",
    "            raise FileNotFoundError(f\"profiles.json not found at {json_path}\")\n",
    "\n",
    "        with open(json_path, \"r\") as f:\n",
    "            self.PROFILES = json.load(f)\n",
    "\n",
    "    def predict(\n",
    "        self,\n",
    "        messages: list[ChatAgentMessage],\n",
    "        context: Optional[ChatContext] = None,\n",
    "        custom_inputs: Optional[dict[str, Any]] = None,\n",
    "    ) -> ChatAgentResponse:\n",
    "        \"\"\"\n",
    "        Uses the loaded profiles.json to generate responses.\n",
    "        \"\"\"\n",
    "        custom_inputs = custom_inputs or {}\n",
    "        segment = custom_inputs.get(\"segment\", \"Casual Users\")\n",
    "        profile = self.PROFILES.get(\n",
    "            segment, \"A casual user doesn't think too much about the product. They will just buy whatever is convenient or cheapest.\")\n",
    "        \n",
    "        request = {\n",
    "            \"messages\": self._convert_messages_to_dict(messages),\n",
    "            **({\"custom_inputs\": custom_inputs} if custom_inputs else {}),\n",
    "            \"context\": {**(context.model_dump_compat() if context else {}), \"profile\": profile},\n",
    "        }\n",
    "\n",
    "        response = ChatAgentResponse(messages=[])\n",
    "        retrieved_ads = \"\"\n",
    "\n",
    "        for event in self.agent.stream(request, stream_mode=\"updates\"):\n",
    "            for node_data in event.values():\n",
    "                if not node_data:\n",
    "                    continue\n",
    "                for msg in node_data.get(\"messages\", []):\n",
    "                    response.messages.append(ChatAgentMessage(**msg))\n",
    "                if \"custom_outputs\" in node_data:\n",
    "                    response.custom_outputs = node_data[\"custom_outputs\"]\n",
    "\n",
    "        return response\n",
    "    \n",
    "    def predict_stream(\n",
    "        self,\n",
    "        messages: list[ChatAgentMessage],\n",
    "        context: Optional[ChatContext] = None,\n",
    "        custom_inputs: Optional[dict[str, Any]] = None,\n",
    "    ) -> Generator[ChatAgentChunk, None, None]:\n",
    "        \"\"\"\n",
    "        Uses the loaded profiles.json to generate responses.\n",
    "        \"\"\"\n",
    "        custom_inputs = custom_inputs or {}\n",
    "        segment = custom_inputs.get(\"segment\", \"Casual Users\")\n",
    "        profile = self.PROFILES.get(\n",
    "            segment, \"A casual user doesn't think too much about the product. They will just buy whatever is convenient or cheapest.\")\n",
    "\n",
    "        request = {\n",
    "            \"messages\": self._convert_messages_to_dict(messages),\n",
    "            **({\"custom_inputs\": custom_inputs} if custom_inputs else {}),\n",
    "            \"context\": {**(context.model_dump_compat() if context else {}), \"profile\": profile},\n",
    "        }\n",
    "\n",
    "        response = ChatAgentResponse(messages=[])\n",
    "        for event in self.agent.stream(request, stream_mode=\"updates\"):\n",
    "            for node_data in event.values():\n",
    "                if not node_data:\n",
    "                    continue\n",
    "                messages = node_data.get(\"messages\", [])\n",
    "                custom_outputs = node_data.get(\"custom_outputs\")\n",
    "                for i, message in enumerate(messages):\n",
    "                    chunk = {\"delta\": message}\n",
    "                    # Only emit custom_outputs with the last streaming chunk from this node\n",
    "                    if custom_outputs and i == len(messages) - 1:\n",
    "                        chunk[\"custom_outputs\"] = custom_outputs\n",
    "                    yield ChatAgentChunk(**chunk)\n",
    "\n",
    "\n",
    "# Create the agent object, and specify it as the agent object to use when\n",
    "# loading the agent back for inference via mlflow.models.set_model()\n",
    "agent = create_profile_agent(llm)\n",
    "AGENT = LangGraphChatAgent(agent)\n",
    "mlflow.models.set_model(AGENT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2bdcfe7c-bb75-4a41-8dc8-8b442a865543",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 3: Test the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3af85400-44de-4a13-b9d8-aa4c2f915004",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8590f63f-2392-4518-b930-cceb537d4fca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./_resources/00_setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4faa8077-a30c-4d92-9ab3-7f0d40115659",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from agent import AGENT\n",
    "\n",
    "input_example = {\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": \"How can I improve this ad? 'Introducing our new laptop with high-end specs and modern design'\"}],\n",
    "        \"custom_inputs\": {\"segment\": \"Young Urban Professional\"},\n",
    "    }\n",
    "\n",
    "AGENT.predict(input_example) # This will use the generic profile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9237d0ff-4d52-4869-b56b-61c5163daa0c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 4: Log the Agent as an MLflow Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14c34204-f21b-42d9-948b-a8b3f866d18f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from agent import LLM_ENDPOINT_NAME, VS_INDEX_NAME\n",
    "from mlflow.models.resources import DatabricksVectorSearchIndex, DatabricksServingEndpoint\n",
    "\n",
    "resources = [\n",
    "    DatabricksVectorSearchIndex(index_name=VS_INDEX_NAME),\n",
    "    DatabricksServingEndpoint(endpoint_name=LLM_ENDPOINT_NAME),\n",
    "    ]\n",
    "\n",
    "with mlflow.start_run():\n",
    "    logged_agent_info = mlflow.pyfunc.log_model(\n",
    "        artifact_path=\"agent\",\n",
    "        python_model=\"agent.py\",\n",
    "        pip_requirements=[\n",
    "            \"mlflow\",\n",
    "            \"langchain\",\n",
    "            \"langgraph\",\n",
    "            \"databricks-langchain\",\n",
    "            \"pydantic\",\n",
    "        ],\n",
    "        resources=resources,\n",
    "        artifacts={\"profiles\": f\"/Volumes/{config['catalog']}/{config['schema']}/{config['profiles_volume']}\"},\n",
    "        input_example=input_example,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "99704fe0-228f-46f5-bdbc-ca4fd66619b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Pre-deployment Agent Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c1bc3c5-0d78-4a25-a9b8-fd3c2e7388cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mlflow.models.predict(\n",
    "    model_uri=f\"runs:/{logged_agent_info.run_id}/agent\",\n",
    "    input_data=input_example,\n",
    ") # This should use the right profile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "46e62454-7905-4101-a20e-88414fe2a9fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 4: Register the Model to Unity Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42cb7469-ae6b-467c-a778-7711c8255208",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "\n",
    "model_name = \"ad_profile_agent\"\n",
    "UC_MODEL_NAME = f\"{config['catalog']}.{config['schema']}.{model_name}\"\n",
    "\n",
    "# register the model to UC\n",
    "uc_registered_model_info = mlflow.register_model(\n",
    "    model_uri=logged_agent_info.model_uri, name=UC_MODEL_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8199ba37-5b68-4f52-9f5a-d5eb1ea58b4a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 5: Deploy the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eda26b54-7daa-4dea-af80-adcd47aa78c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks import agents\n",
    "agents.deploy(UC_MODEL_NAME, uc_registered_model_info.version)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3822719765938087,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "04_profile_agents",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
