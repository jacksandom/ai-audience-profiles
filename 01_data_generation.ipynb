{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c5550f5-f066-400d-8fa2-d448b99c5026",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "544a9f79-a8b2-4c79-b347-2bc4fbc91a87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install faker==\"36.1.1\"\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14ebf725-0d6e-408c-8d83-edb0821ba9e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyspark.sql.functions as F\n",
    "import numpy as np\n",
    "import random\n",
    "import uuid\n",
    "\n",
    "from datetime import datetime\n",
    "from faker import Faker\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import uuid\n",
    "import random\n",
    "from faker import Faker\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e56734e4-e157-4eb0-be95-658ed4f4b919",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./_resources/00_setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35a6a737-cbc0-4b1e-95e6-0cf32d19d74e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 1: Generate structured data for clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8af66bab-c561-4835-85b9-f3d0449df3a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Introduction to Demographic Clustering\n",
    "Demographic clustering involves categorizing individuals based on characteristics like age, income, and education level. This process helps in understanding and targeting specific groups more effectively.\n",
    "\n",
    "## Defining Clusters\n",
    "We define several demographic clusters based on their characteristics:\n",
    "\n",
    "- **Young Urban Professional**\n",
    "- **Suburban Family-Oriented**\n",
    "- **Retired Rural Dweller**\n",
    "- **College Student**\n",
    "- **High-Income Empty Nester**\n",
    "\n",
    "## LA Tribes\n",
    "Additionally, we focus on specific tribes in Los Angeles:\n",
    "\n",
    "- **Young Professional Women - Urban Explorers**\n",
    "- **Tech-Savvy Professionals - Silicon Beach Innovators**\n",
    "- **Creative Entrepreneurs - Hollywood Creatives**\n",
    "- **Eco-Conscious Millennials - Sustainable Lifestyle**\n",
    "- **Luxury-Oriented Professionals - Upscale Lifestyles**\n",
    "- **College Students - Campus Life**\n",
    "\n",
    "## Code Implementation\n",
    "\n",
    "Below is the Python code to generate these demographic clusters and tribes:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41e7d271-1442-4836-8b99-5f478c593440",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Set seed\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7fd8dbf-398e-4dea-a1c2-a07096feae82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We need to use conditional probabilities in our data gen code in order to \"force\" the clusters for later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e50c58af-b1d9-4def-be81-68d202e52534",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Initialize Faker for generating fake dates\n",
    "fake = Faker()\n",
    "\n",
    "# Define the tribe sizes and their descriptions\n",
    "tribe_sizes = {\n",
    "    \"Young Professional Women - Urban Explorers\": 250,\n",
    "    \"Tech-Savvy Professionals - Silicon Beach Innovators\": 200,\n",
    "    \"Creative Entrepreneurs - Hollywood Creatives\": 150,\n",
    "    \"Eco-Conscious Millennials - Sustainable Lifestyle\": 180,\n",
    "    \"Luxury-Oriented Professionals - Upscale Lifestyles\": 200,\n",
    "    \"College Students - Campus Life (UCLA / USC)\": 300,\n",
    "}\n",
    "\n",
    "# Define the tribe sizes and their descriptions\n",
    "tribe_sizes_location = {\n",
    "    \"Young Professional Women - Urban Explorers (Downtown LA / Arts District)\": 250,\n",
    "    \"Tech-Savvy Professionals - Silicon Beach Innovators (Venice Beach / Santa Monica)\": 200,\n",
    "    \"Creative Entrepreneurs - Hollywood Creatives (West Hollywood / Beverly Hills)\": 150,\n",
    "    \"Eco-Conscious Millennials - Sustainable Lifestyle (Culver City / Echo Park)\": 180,\n",
    "    \"Luxury-Oriented Professionals - Upscale Lifestyles (Beverly Hills / Bel Air)\": 200,\n",
    "    \"College Students - Campus Life (UCLA / USC)\": 300,\n",
    "}\n",
    "# Function to generate data for LA-based tribes\n",
    "def generate_la_tribe_data(tribe_name, size):\n",
    "    if \"Urban Explorers\" in tribe_name:\n",
    "        locations = random.choices([\"Downtown LA\", \"Arts District\", \"Silver Lake\", \"Echo Park\"], k=size)\n",
    "        incomes = np.random.normal(45000, 12000, size).clip(30000, 80000)\n",
    "        occupations = random.choices([\"Freelancer\", \"Graphic Designer\", \"Social Media Influencer\"], k=size)\n",
    "        ages = np.random.randint(25, 35, size)\n",
    "        education_levels = np.random.choice([\"Bachelor's\", \"Post Graduate\"], size, p=[0.7, 0.3])\n",
    "        relationship_statuses = random.choices([\"Single\", \"Cohabiting\"], k=size)\n",
    "        number_dependants = [0] * size\n",
    "        short_description = \"Downtown LA professionals\"\n",
    "\n",
    "    elif \"Silicon Beach Innovators\" in tribe_name:\n",
    "        locations = random.choices([\"Venice Beach\", \"Santa Monica\", \"Playa Vista\"], k=size)\n",
    "        incomes = np.random.normal(100000, 25000, size).clip(70000, 200000)\n",
    "        occupations = random.choices([\"Software Engineer\", \"Product Manager\", \"UX/UI Designer\"], k=size)\n",
    "        ages = np.random.randint(25, 40, size)\n",
    "        education_levels = np.random.choice([\"Bachelor's\", \"Post Graduate\"], size, p=[0.8, 0.2])\n",
    "        relationship_statuses = random.choices([\"Single\", \"Cohabiting\"], k=size)\n",
    "        number_dependants = [0] * size\n",
    "        short_description = \"Venice Beach tech industry leaders\"\n",
    "\n",
    "    elif \"Hollywood Creatives\" in tribe_name:\n",
    "        locations = random.choices([\"West Hollywood\", \"Beverly Hills\", \"Sunset Strip\"], k=size)\n",
    "        incomes = np.random.normal(120000, 30000, size).clip(80000, 250000)\n",
    "        occupations = random.choices([\"Filmmaker\", \"Designer\", \"Fashion Entrepreneur\"], k=size)\n",
    "        ages = np.random.randint(25, 45, size)\n",
    "        education_levels = np.random.choice([\"Bachelor's\", \"Post Graduate\"], size, p=[0.6, 0.4])\n",
    "        relationship_statuses = random.choices([\"Single\", \"Cohabiting\"], k=size)\n",
    "        number_dependants = [0] * size\n",
    "        short_description = \"West Hollywood creatives\"\n",
    "\n",
    "    elif \"Sustainable Lifestyle\" in tribe_name:\n",
    "        locations = random.choices([\"Culver City\", \"Echo Park\", \"Silver Lake\"], k=size)\n",
    "        incomes = np.random.normal(55000, 15000, size).clip(40000, 100000)\n",
    "        occupations = random.choices([\"Sustainability Consultant\", \"Eco-Entrepreneur\", \"Health Coach\"], k=size)\n",
    "        ages = np.random.randint(25, 35, size)\n",
    "        education_levels = np.random.choice([\"Bachelor's\", \"Post Graduate\"], size, p=[0.7, 0.3])\n",
    "        relationship_statuses = random.choices([\"Single\", \"Cohabiting\"], k=size)\n",
    "        number_dependants = [0] * size\n",
    "        short_description = \"Culver City eco-advocates\"\n",
    "\n",
    "    elif \"Luxury-Oriented Professionals\" in tribe_name:\n",
    "        locations = random.choices([\"Beverly Hills\", \"Bel Air\", \"Westwood\"], k=size)\n",
    "        incomes = np.random.normal(200000, 50000, size).clip(150000, 300000)\n",
    "        occupations = random.choices([\"Real Estate Executive\", \"Investment Banker\", \"Entertainment Lawyer\"], k=size)\n",
    "        ages = np.random.randint(35, 55, size)\n",
    "        education_levels = np.random.choice([\"Bachelor's\", \"Post Graduate\"], size, p=[0.6, 0.4])\n",
    "        relationship_statuses = random.choices([\"Single\", \"Cohabiting\"], k=size)\n",
    "        number_dependants = [0] * size\n",
    "        short_description = \"Beverly Hills luxury professionals\"\n",
    "\n",
    "    elif \"College Students\" in tribe_name:\n",
    "        locations = random.choices([\"Westwood (UCLA)\", \"South LA (USC)\", \"Downtown LA\"], k=size)\n",
    "        incomes = np.random.normal(15000, 3000, size).clip(10000, 25000)\n",
    "        occupations = random.choices([\"Student\", \"Intern\", \"Part-time Worker\"], k=size)\n",
    "        ages = np.random.randint(18, 24, size)\n",
    "        education_levels = [\"Some College\"] * size\n",
    "        relationship_statuses = [\"Single\"] * size\n",
    "        number_dependants = [0] * size\n",
    "        short_description = \"UCLA/USC students\"\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        \"age\": ages,\n",
    "        \"income\": incomes.round(-3),\n",
    "        \"location\": locations,\n",
    "        \"education\": education_levels,\n",
    "        \"relationship_status\": relationship_statuses,\n",
    "        \"number_dependants\": number_dependants,\n",
    "        \"occupation\": occupations,\n",
    "        \"tribe\": tribe_name,\n",
    "        \"short_description\": short_description\n",
    "    })\n",
    "\n",
    "# Generate data for all LA tribes\n",
    "tribe_dfs = [generate_la_tribe_data(tribe, size) for tribe, size in tribe_sizes.items()]\n",
    "demographic_df = pd.concat(tribe_dfs, ignore_index=True)\n",
    "\n",
    "# Shuffle the dataset\n",
    "demographic_df = demographic_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Add UUIDs for each individual\n",
    "demographic_df.insert(0, 'uuid', [str(uuid.uuid4()) for _ in range(len(demographic_df))])\n",
    "\n",
    "# Add fake creation dates for each record (simulating when they joined the platform)\n",
    "demographic_df['created_at'] = [\n",
    "  fake.date_time_between(datetime(2023, 1, 1), datetime(2024, 12, 31)).strftime('%Y-%m-%d %H:%M:%S') for _ in range(len(demographic_df))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8c46e72-3a3a-4f2c-a321-80d65bda031a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Display the first few records of the generated dataset\n",
    "\n",
    "display(demographic_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5fdbf68-8f20-4522-8c49-853820c95f71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "# Set your Mapbox access token\n",
    "px.set_mapbox_access_token(\"ADD TOKEN\")\n",
    "\n",
    "# Define location coordinates\n",
    "location_coords = {\n",
    "    \"Downtown LA\": (34.0522, -118.2437),\n",
    "    \"Arts District\": (34.0403, -118.2352),\n",
    "    \"Silver Lake\": (34.0872, -118.2707),\n",
    "    \"Echo Park\": (34.0782, -118.2606),\n",
    "    \"Venice Beach\": (33.9850, -118.4695),\n",
    "    \"Santa Monica\": (34.0195, -118.4912),\n",
    "    \"Playa Vista\": (33.9754, -118.4208),\n",
    "    \"West Hollywood\": (34.0900, -118.3617),\n",
    "    \"Beverly Hills\": (34.0736, -118.4004),\n",
    "    \"Sunset Strip\": (34.0928, -118.3854),\n",
    "    \"Culver City\": (34.0211, -118.3965),\n",
    "    \"Bel Air\": (34.1000, -118.4614),\n",
    "    \"Westwood\": (34.0561, -118.4290),\n",
    "    \"Westwood (UCLA)\": (34.0689, -118.4452),\n",
    "    \"South LA (USC)\": (34.0224, -118.2851)\n",
    "}\n",
    "\n",
    "# Add latitude and longitude to the dataframe\n",
    "demographic_df['latitude'] = demographic_df['location'].map(lambda x: location_coords.get(x.split('(')[0].strip(), (None, None))[0])\n",
    "demographic_df['longitude'] = demographic_df['location'].map(lambda x: location_coords.get(x.split('(')[0].strip(), (None, None))[1])\n",
    "\n",
    "# Check if latitude and longitude columns were added correctly\n",
    "if demographic_df[['latitude', 'longitude']].isnull().any().any():\n",
    "    print(\"Warning: Some locations do not have coordinates in the dictionary.\")\n",
    "    missing_locations = set(demographic_df['location'].map(lambda x: x.split('(')[0].strip())) - set(location_coords.keys())\n",
    "    print(\"Missing locations:\", missing_locations)\n",
    "\n",
    "demographic_df = demographic_df.dropna(subset=['latitude', 'longitude'])\n",
    "\n",
    "fig = px.scatter_mapbox(\n",
    "    demographic_df,\n",
    "    lat=\"latitude\",\n",
    "    lon=\"longitude\",\n",
    "    color=\"tribe\",\n",
    "    size=\"income\",\n",
    "    hover_name=\"short_description\",\n",
    "    hover_data=[\"age\", \"occupation\", \"education\"],\n",
    "    zoom=10,\n",
    "    height=600\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    mapbox_style=\"mapbox://styles/mapbox/streets-v12\",\n",
    "    margin={\"r\": 0, \"t\": 0, \"l\": 0, \"b\": 0},\n",
    "    legend=dict(\n",
    "        orientation=\"h\",  # Horizontal orientation\n",
    "        yanchor=\"bottom\",  # Position at the bottom\n",
    "        y=1.02,  # Slightly above the bottom\n",
    "        xanchor=\"right\",  # Align to the right\n",
    "        x=1,  # Position at the right edge\n",
    "        font=dict(size=8)  # Reduce font size\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77068db6-d72d-4c1b-8751-bb80b0ddbc2d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 2: Generate social media posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c9b4015-f57e-4d37-9532-fc0e5e280e56",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fake = Faker()\n",
    "\n",
    "# Updated Tribes based on your focus\n",
    "tribes = {\n",
    "    \"Young Professional Women - Urban Explorers\": {\n",
    "        \"products\": [\"travel bag\", \"stylish sunglasses\", \"compact camera\", \"smartphone\", \"fitness tracker\"],\n",
    "        \"emotions\": [\"excited\", \"curious\", \"adventurous\", \"relaxed\"]\n",
    "    },\n",
    "    \"Tech-Savvy Professionals - Silicon Beach Innovators\": {\n",
    "        \"products\": [\"smartphone\", \"laptop\", \"smartwatch\", \"tablet\", \"wireless earbuds\"],\n",
    "        \"emotions\": [\"excited\", \"satisfied\", \"curious\", \"innovative\"]\n",
    "    },\n",
    "    \"Creative Entrepreneurs - Hollywood Creatives\": {\n",
    "        \"products\": [\"designer laptop\", \"high-end camera\", \"smartphone\", \"creative tools\", \"luxury watch\"],\n",
    "        \"emotions\": [\"inspired\", \"excited\", \"content\", \"creative\"]\n",
    "    },\n",
    "    \"Eco-Conscious Millennials - Sustainable Lifestyle\": {\n",
    "        \"products\": [\"reusable water bottle\", \"organic skincare\", \"bamboo toothbrush\", \"electric bike\", \"solar-powered charger\"],\n",
    "        \"emotions\": [\"satisfied\", \"relaxed\", \"eco-friendly\", \"content\"]\n",
    "    },\n",
    "    \"Luxury-Oriented Professionals - Upscale Lifestyles\": {\n",
    "        \"products\": [\"luxury watch\", \"high-end camera\", \"luxury car\", \"premium wine\", \"gourmet food subscription\"],\n",
    "        \"emotions\": [\"satisfied\", \"relaxed\", \"luxurious\", \"content\"]\n",
    "    },\n",
    "    \"College Students - Campus Life (UCLA / USC)\": {\n",
    "        \"products\": [\"backpack\", \"laptop\", \"coffee maker\", \"textbooks\", \"headphones\"],\n",
    "        \"emotions\": [\"excited\", \"curious\", \"satisfied\", \"motivated\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "# Get random sample from demographic data (assuming `demographic_df` is already created)\n",
    "sampled_df = demographic_df.sample(n=100).reset_index(drop=True)\n",
    "\n",
    "# Generate combinations for 100 unique posts\n",
    "combinations = []\n",
    "unique_combinations = set()\n",
    "\n",
    "while len(unique_combinations) < 100:\n",
    "    # Select a tribe randomly from the dictionary\n",
    "    tribe_name = random.choice(list(tribes.keys()))\n",
    "    \n",
    "    # Select a random author_id from the demographic data based on the tribe (segment)\n",
    "    author_id = demographic_df[demographic_df[\"tribe\"] == tribe_name][\"uuid\"].sample(1).values[0]\n",
    "    \n",
    "    # Choose a product and emotion from the tribe\n",
    "    product = random.choice(tribes[tribe_name][\"products\"])\n",
    "    emotion = random.choice(tribes[tribe_name][\"emotions\"])\n",
    "    \n",
    "    # Create a tuple to check for uniqueness\n",
    "    combination_tuple = (tribe_name, product, emotion)\n",
    "    \n",
    "    # Add only if the combination is unique\n",
    "    if combination_tuple not in unique_combinations:\n",
    "        unique_combinations.add(combination_tuple)\n",
    "        combinations.append({\n",
    "            \"author_id\": author_id,\n",
    "            \"tribe\": tribe_name,\n",
    "            \"product\": product,\n",
    "            \"emotion\": emotion\n",
    "        })\n",
    "\n",
    "combinations_df = pd.DataFrame(combinations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "180a15ba-0dd7-4041-a6d9-af1781f2dfae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "combinations_sdf = spark.createDataFrame(combinations_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c63ba6b-0070-4082-8247-f0e930f8e1c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(combinations_sdf.groupBy(\"tribe\").count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "673309bd-6353-4a87-b1df-938a75cdceb4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Creat temp view for AI_QUERY\n",
    "combinations_sdf.createOrReplaceTempView(\"sampled_audience\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6f0c7e0-4bf4-4d03-8fe3-4e304f85e1c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create the SQL query for generating social media posts\n",
    "sql_query = \"\"\"\n",
    "CREATE OR REPLACE TEMP VIEW sampled_audience_posts AS\n",
    "SELECT\n",
    "    author_id,\n",
    "    tribe,\n",
    "    product,\n",
    "    emotion,\n",
    "    AI_QUERY(\n",
    "        \"databricks-meta-llama-3-3-70b-instruct\", \n",
    "        CONCAT(\n",
    "            'Generate a realistic social media post from a consumer who recently purchased a ',\n",
    "            product, \n",
    "            ' from the perspective of a ', tribe, \n",
    "            ' who is ', emotion, \n",
    "            ' about the product. The post should reflect their genuine experience, including specific details about the product\\'s features, performance, and how it fits into their lifestyle. Maintain a conversational and engaging tone, similar to how people naturally write on social media. Optionally, include a hashtag or emoji for authenticity. Don\\'t explicitly mention the tribe or that you are an AI assistant. Remove quotation marks.'\n",
    "        ) AS post\n",
    "FROM sampled_audience\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "146b79cc-5988-4ae2-8083-9009aeae22ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "posts_df = spark.sql(\"SELECT * FROM sampled_audience\").toPandas()\n",
    "display(posts_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b7f0dbf-735c-471a-a1b9-4726be4af0d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 3: Generate Ad Campaigns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d422c930-c171-4d49-a584-1bc39ce132de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Add fake creation dates for the posts\n",
    "posts_df['created_at'] = [fake.date_time_between(datetime(2023, 1, 1), datetime(2024, 12, 31)).strftime('%Y-%m-%d %H:%M:%S') for _ in range(len(posts_df))]\n",
    "\n",
    "# Optionally: Save posts to a JSON or database\n",
    "posts_df.to_json('social_media_posts.json', orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d3c6d3e-0e82-4a05-8e53-425b6f911c9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Generate Ad Campaigns\n",
    "campaigns = []\n",
    "for tribe, details in tribes.items():\n",
    "    for product in details[\"products\"]:\n",
    "        campaign = {\n",
    "            \"campaign_id\": f\"AD-{random.randint(1000,9999)}\",\n",
    "            \"segment\": tribe,\n",
    "            \"product\": product,\n",
    "            \"tone\": random.choice(details[\"emotions\"]),\n",
    "            \"cta\": random.choice([\"Shop now\", \"Learn more\", \"Get yours\", \"Discover\"]),\n",
    "            \"impressions\": random.randint(10000, 500000),\n",
    "            \"ctr\": round(random.uniform(0.5, 5.0), 2)\n",
    "        }\n",
    "        campaigns.append(campaign)\n",
    "\n",
    "campaigns_df = pd.DataFrame(campaigns)\n",
    "\n",
    "# Add campaign dates\n",
    "campaigns_df['start_date'] = [fake.date_between(datetime(2023,1,1), datetime(2024,1,1)) for _ in range(len(campaigns_df))]\n",
    "campaigns_df['end_date'] = [fake.date_between(datetime(2024,1,1), datetime(2024,12,31)) for _ in range(len(campaigns_df))]\n",
    "\n",
    "# Create Spark DataFrame for campaigns\n",
    "campaigns_sdf = spark.createDataFrame(campaigns_df)\n",
    "campaigns_sdf.createOrReplaceTempView(\"campaigns\")\n",
    "\n",
    "# Generate AI-optimized ad copies\n",
    "performance_query = \"\"\"\n",
    "CREATE OR REPLACE TEMP VIEW campaigns_performance AS\n",
    "SELECT\n",
    "    campaign_id,\n",
    "    segment,\n",
    "    product,\n",
    "    AI_QUERY(\n",
    "        \"databricks-meta-llama-3-3-70b-instruct\", \n",
    "        CONCAT(\n",
    "            'Create a digital ad for ', product, \n",
    "            ' targeting ', segment, \n",
    "            ' with ', tone, ' tone. Include: ',\n",
    "            '- Key product benefits\\n',\n",
    "            '- Lifestyle connection\\n',\n",
    "            '- Emoji if appropriate\\n',\n",
    "            '- CTA: ', cta, '\\n',\n",
    "            '- Max 200 characters\\n',\n",
    "            '- No hashtags\\n',\n",
    "            '- Natural conversational style'\n",
    "        )\n",
    "    ) AS optimized_ad_copy,\n",
    "    impressions,\n",
    "    ctr\n",
    "FROM campaigns\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(performance_query)\n",
    "campaigns_performance_df = spark.sql(\"SELECT * FROM campaigns_performance\").toPandas()\n",
    "\n",
    "# Add performance metrics\n",
    "campaigns_performance_df['estimated_clicks'] = (campaigns_performance_df['impressions'] * \n",
    "                                               campaigns_performance_df['ctr']/100).astype(int)\n",
    "\n",
    "# Display results\n",
    "print(\"Generated Social Media Posts:\")\n",
    "display(posts_df)\n",
    "\n",
    "print(\"\\nAI-Optimized Campaign Performance:\")\n",
    "display(campaigns_performance_df[['campaign_id', 'segment', 'optimized_ad_copy', \n",
    "                                'impressions', 'ctr', 'estimated_clicks']])\n",
    "\n",
    "# Save to JSON\n",
    "posts_df.to_json('social_media_posts.json', orient='records', indent=2)\n",
    "campaigns_performance_df.to_json('ai_optimized_campaigns.json', orient='records', indent=2)\n",
    "\n",
    "print(\"Data saved to 'social_media_posts.json' and 'ai_optimized_campaigns.json'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d92251d9-1990-4d6c-9036-86d81538d227",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Write social media posts to volume JSON and save demographic + campaign tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8c20172-73b8-48fd-874a-262c6e4555e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the path to save social media feed\n",
    "catalog = \"marca_tribes\"\n",
    "schema = \"ai_audience_segments\"\n",
    "socials_volume = \"social_media_feed\"\n",
    "vol_social_media_feed = f\"/mnt/{catalog}/{schema}/{socials_volume}/posts.json\"\n",
    "\n",
    "# Check if the directory exists, and create it if not\n",
    "dbutils.fs.mkdirs(f\"/mnt/{catalog}/{schema}/{socials_volume}\")\n",
    "\n",
    "# Convert the Pandas DataFrame to a Spark DataFrame if necessary\n",
    "if isinstance(posts_df, pd.DataFrame):\n",
    "    posts_df = spark.createDataFrame(posts_df)\n",
    "\n",
    "# Save the social media posts DataFrame to the volume as a JSON file\n",
    "posts_df.write.json(vol_social_media_feed, mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0eef12d6-7d6f-4543-97d6-452cb7bbb838",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write demographic data to UC table dropping segment\n",
    "demographic_sdf = spark.createDataFrame(demographic_df)\n",
    "demographic_sdf = demographic_sdf.drop(\"segment\")\n",
    "\n",
    "# Define the catalog and schema\n",
    "catalog = \"marca_tribes\"\n",
    "schema = \"ai_audience_segments\"\n",
    "table_name = \"audience_demographic\"\n",
    "\n",
    "# Save the DataFrame as a Delta table in Unity Catalog\n",
    "demographic_sdf.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{catalog}.{schema}.{table_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e141aea-3dad-4882-b62a-42cfced3193a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write campaigns data to UC table\n",
    "campaigns_df = spark.createDataFrame(campaigns_performance_df)\n",
    "\n",
    "# Define the catalog and schema\n",
    "catalog = \"marca_tribes\"\n",
    "schema = \"ai_audience_segments\"\n",
    "table_name = \"campaigns_performance\"\n",
    "\n",
    "# Save the DataFrame as a Delta table in Unity Catalog\n",
    "campaigns_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{catalog}.{schema}.{table_name}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3987948637745141,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "01_data_generation",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
