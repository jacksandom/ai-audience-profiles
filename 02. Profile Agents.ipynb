{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d3fadff9-f023-49bb-be6e-9f4ed2295ba8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Profile Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5f9ed78-e7b8-421b-a0f1-755b4e17c10f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %pip install -U -qqqq mlflow langchain langgraph databricks-langchain pydantic databricks-agents \n",
    "# dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ffcb8e7-ec1d-4f03-82da-6eeb1560b656",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ae40962-2023-419b-b936-51861104804e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "# Get profiles and save locally\n",
    "###############################\n",
    "with open(f\"/Volumes/jack_sandom/ai_audience_segments/profiles/profiles.json\", \"r\") as f:\n",
    "  PROFILES = json.load(f)\n",
    "\n",
    "os.makedirs(\"model_artifacts\", exist_ok=True)\n",
    "json_path = \"model_artifacts/profiles.json\"\n",
    "\n",
    "with open(json_path, \"w\") as f:\n",
    "    json.dump(PROFILES, f)\n",
    "\n",
    "print(f\"Profiles JSON saved locally at {json_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6f7576c9-580c-43bc-9384-9550624a89ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 1: Define the Agent in Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a583b2aa-e1b1-4507-a6af-7dd6beef9e1b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%%writefile agent.py\n",
    "from typing import Any, Generator, Optional, Sequence, Union\n",
    "\n",
    "import json\n",
    "import os\n",
    "import mlflow\n",
    "from databricks_langchain import ChatDatabricks\n",
    "from langchain_core.language_models import LanguageModelLike\n",
    "from langchain_core.runnables import RunnableConfig, RunnableLambda\n",
    "from langchain_core.tools import BaseTool\n",
    "from langgraph.graph import END, StateGraph\n",
    "from langgraph.graph.graph import CompiledGraph\n",
    "from langgraph.graph.state import CompiledStateGraph\n",
    "from langchain.prompts import PromptTemplate\n",
    "from mlflow.langchain.chat_agent_langgraph import ChatAgentState, ChatAgentToolNode\n",
    "from mlflow.pyfunc import ChatAgent\n",
    "from mlflow.types.agent import (\n",
    "    ChatAgentChunk,\n",
    "    ChatAgentMessage,\n",
    "    ChatAgentResponse,\n",
    "    ChatContext,\n",
    ")\n",
    "\n",
    "mlflow.langchain.autolog()\n",
    "\n",
    "############################################\n",
    "# Define your LLM endpoint and system prompt\n",
    "############################################\n",
    "LLM_ENDPOINT_NAME = \"databricks-meta-llama-3-3-70b-instruct\"\n",
    "llm = ChatDatabricks(endpoint=LLM_ENDPOINT_NAME)\n",
    "\n",
    "system_prompt = PromptTemplate(\n",
    "    input_variables=[\"segment\", \"profile\"],\n",
    "    template=\"\"\"\n",
    "    You are an audience persona named {segment} with the following profile:\n",
    "    {profile}\n",
    "\n",
    "    The user is an advertising content writer and wants to tailor copy specific to your persona. Your goal is to assist the user in doing this by acting as a {segment} and helping the user to test ideas.\n",
    "\n",
    "    If asked to improve a specific piece of ad conent, provide 3-5 actionable concise recommendations to make this ad more appealing to the customer profile. Give an example of an improved ad text.\n",
    "    Your response should follow this structured format:\n",
    "    \n",
    "    - **Highlight Key Features:** (What should be emphasised?)\n",
    "    - **Tone Adjustments:** (How should the messaging be modified?)\n",
    "    - **Messaging Strategies:** (What persuasive elements should be included?)\n",
    "\n",
    "    **Improved ad text**\n",
    "\n",
    "    Do not do this unless asked to do so.\n",
    "\n",
    "    Stay in character always and respond to questions as this persona. Only respond in the context of your audience persona but don't refer to yourself by the segment name. If asked about something unrelated, politely redirect the conversation.\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "tools = []\n",
    "\n",
    "JSON_PATH = \"model_artifacts/profiles.json\"  # Local path before MLflow logging\n",
    "\n",
    "#####################\n",
    "## Define agent logic\n",
    "#####################\n",
    "\n",
    "def get_customer_profile(custom_inputs, profiles):\n",
    "    \"\"\"\n",
    "    Retrieves a predefined customer profile based on the segment.\n",
    "    If provided segment is invalid, chooses a default profile.\n",
    "    \"\"\"\n",
    "    segment = custom_inputs.get(\"segment\", \"Casual Users\")\n",
    "    return profiles.get(segment, \"No profile available.\")\n",
    "\n",
    "\n",
    "def create_profile_agent(\n",
    "    model: LanguageModelLike,\n",
    "    agent_prompt: Optional[str] = None,\n",
    ") -> CompiledGraph:\n",
    "    model = model.bind_tools(tools)\n",
    "\n",
    "    # Define the function that determines which node to go to\n",
    "    def should_continue(state: ChatAgentState):\n",
    "        messages = state[\"messages\"]\n",
    "        last_message = messages[-1]\n",
    "        # If there are function calls, continue. else, end\n",
    "        return \"continue\" if last_message.get(\"tool_calls\") else \"end\"\n",
    "    \n",
    "    def generate_prompt_with_profile(state: ChatAgentState):\n",
    "        \"\"\"\n",
    "        Retrieves the customer profile and formats the system prompt dynamically.\n",
    "        \"\"\"\n",
    "        custom_inputs = state.get(\"custom_inputs\", {})\n",
    "        profile = get_customer_profile(custom_inputs, state[\"context\"].get(\"profiles\", {}))\n",
    "\n",
    "        formatted_prompt = system_prompt.format(\n",
    "            segment=custom_inputs.get(\"segment\", \"Casual Users\"),\n",
    "            profile=profile\n",
    "        )\n",
    "\n",
    "        # Store the profile in context so it persists during the chat\n",
    "        state[\"context\"][\"customer_profile\"] = profile\n",
    "\n",
    "        return [{\"role\": \"system\", \"content\": formatted_prompt}] + state[\"messages\"]\n",
    "\n",
    "    preprocessor = RunnableLambda(generate_prompt_with_profile)\n",
    "    model_runnable = preprocessor | model\n",
    "\n",
    "\n",
    "    def call_model(\n",
    "        state: ChatAgentState,\n",
    "        config: RunnableConfig,\n",
    "    ):\n",
    "        response = model_runnable.invoke(state, config)\n",
    "\n",
    "        return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "    workflow = StateGraph(ChatAgentState)\n",
    "    workflow.add_node(\"agent\", RunnableLambda(call_model))\n",
    "    workflow.set_entry_point(\"agent\")\n",
    "\n",
    "    return workflow.compile()\n",
    "\n",
    "\n",
    "class LangGraphChatAgent(ChatAgent, mlflow.pyfunc.PythonModel):\n",
    "    def __init__(self, agent: CompiledStateGraph, profiles_path: str = None):\n",
    "        self.agent = agent\n",
    "        self.PROFILES = {}\n",
    "\n",
    "        # Load profiles locally if available (before logging the model)\n",
    "        if profiles_path and os.path.exists(profiles_path):\n",
    "            print(f\"âœ… Loading profiles from local JSON at {profiles_path}\")\n",
    "            with open(profiles_path, \"r\") as f:\n",
    "                self.PROFILES = json.load(f)\n",
    "        else:\n",
    "            print(f\"profiles.json not found locally. Will load from context.\")\n",
    "\n",
    "    def load_context(self, context):\n",
    "        \"\"\"\n",
    "        Loads customer profiles from MLflow artifacts when the model is served.\n",
    "        \"\"\"\n",
    "        config_path = context.artifacts.get(\"config\")\n",
    "        json_path = os.path.join(config_path, \"profiles.json\")\n",
    "\n",
    "        if not os.path.exists(json_path):\n",
    "            raise FileNotFoundError(f\"profiles.json not found at {json_path}\")\n",
    "\n",
    "        with open(json_path, \"r\") as f:\n",
    "            self.PROFILES = json.load(f)\n",
    "\n",
    "    def predict(\n",
    "        self,\n",
    "        messages: list[ChatAgentMessage],\n",
    "        context: Optional[ChatContext] = None,\n",
    "        custom_inputs: Optional[dict[str, Any]] = None,\n",
    "    ) -> ChatAgentResponse:\n",
    "        \"\"\"\n",
    "        Uses the loaded profiles.json to generate responses.\n",
    "        \"\"\"\n",
    "        custom_inputs = custom_inputs or {}\n",
    "        segment = custom_inputs.get(\"segment\", \"Casual Users\")\n",
    "        profile = self.PROFILES.get(segment, \"No profile available.\")\n",
    "\n",
    "        request = {\n",
    "            \"messages\": self._convert_messages_to_dict(messages),\n",
    "            \"custom_inputs\": custom_inputs,\n",
    "            \"context\": context.model_dump_compat() if context else {},\n",
    "        }\n",
    "\n",
    "        response = ChatAgentResponse(messages=[])\n",
    "        for event in self.agent.stream(request, stream_mode=\"updates\"):\n",
    "            for node_data in event.values():\n",
    "                if not node_data:\n",
    "                    continue\n",
    "                for msg in node_data.get(\"messages\", []):\n",
    "                    response.messages.append(ChatAgentMessage(**msg))\n",
    "                if \"custom_outputs\" in node_data:\n",
    "                    response.custom_outputs = node_data[\"custom_outputs\"]\n",
    "        return response\n",
    "    \n",
    "    def predict_stream(\n",
    "        self,\n",
    "        messages: list[ChatAgentMessage],\n",
    "        context: Optional[ChatContext] = None,\n",
    "        custom_inputs: Optional[dict[str, Any]] = None,\n",
    "    ) -> Generator[ChatAgentChunk, None, None]:\n",
    "        \"\"\"\n",
    "        Uses the loaded profiles.json to generate responses.\n",
    "        \"\"\"\n",
    "        custom_inputs = custom_inputs or {}\n",
    "        segment = custom_inputs.get(\"segment\", \"Casual Users\")\n",
    "        profile = self.PROFILES.get(segment, \"No profile available.\")\n",
    "\n",
    "        request = {\n",
    "            \"messages\": self._convert_messages_to_dict(messages),\n",
    "            \"custom_inputs\": custom_inputs,\n",
    "            \"context\": context.model_dump_compat() if context else {},\n",
    "        }\n",
    "\n",
    "        response = ChatAgentResponse(messages=[])\n",
    "        for event in self.agent.stream(request, stream_mode=\"updates\"):\n",
    "            for node_data in event.values():\n",
    "                if not node_data:\n",
    "                    continue\n",
    "                messages = node_data.get(\"messages\", [])\n",
    "                custom_outputs = node_data.get(\"custom_outputs\")\n",
    "                for i, message in enumerate(messages):\n",
    "                    chunk = {\"delta\": message}\n",
    "                    # Only emit custom_outputs with the last streaming chunk from this node\n",
    "                    if custom_outputs and i == len(messages) - 1:\n",
    "                        chunk[\"custom_outputs\"] = custom_outputs\n",
    "                    yield ChatAgentChunk(**chunk)\n",
    "\n",
    "\n",
    "# Create the agent object, and specify it as the agent object to use when\n",
    "# loading the agent back for inference via mlflow.models.set_model()\n",
    "agent = create_profile_agent(llm, system_prompt)\n",
    "AGENT = LangGraphChatAgent(agent, JSON_PATH)\n",
    "mlflow.models.set_model(AGENT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2bdcfe7c-bb75-4a41-8dc8-8b442a865543",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 2: Test the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3af85400-44de-4a13-b9d8-aa4c2f915004",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4faa8077-a30c-4d92-9ab3-7f0d40115659",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from agent import AGENT\n",
    "\n",
    "input_example = {\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": \"Hello, what do you look for in products?\"}],\n",
    "        \"custom_inputs\": {\"segment\": \"Young Urban Professional\"},\n",
    "    }\n",
    "\n",
    "AGENT.predict(input_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9237d0ff-4d52-4869-b56b-61c5163daa0c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 3: Log the Agent as an MLflow Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14c34204-f21b-42d9-948b-a8b3f866d18f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from agent import tools, LLM_ENDPOINT_NAME\n",
    "from mlflow.models.resources import DatabricksFunction, DatabricksServingEndpoint\n",
    "\n",
    "resources = [DatabricksServingEndpoint(endpoint_name=LLM_ENDPOINT_NAME)]\n",
    "\n",
    "with mlflow.start_run():\n",
    "    logged_agent_info = mlflow.pyfunc.log_model(\n",
    "        artifact_path=\"agent\",\n",
    "        python_model=\"agent.py\",\n",
    "        pip_requirements=[\n",
    "            \"mlflow\",\n",
    "            \"langchain\",\n",
    "            \"langgraph\",\n",
    "            \"databricks-langchain\",\n",
    "            \"pydantic\",\n",
    "        ],\n",
    "        resources=resources,\n",
    "        artifacts={\"config\": \"model_artifacts\"},\n",
    "        input_example=input_example,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "99704fe0-228f-46f5-bdbc-ca4fd66619b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Pre-deployment Agent Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c1bc3c5-0d78-4a25-a9b8-fd3c2e7388cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mlflow.models.predict(\n",
    "    model_uri=f\"runs:/{logged_agent_info.run_id}/agent\",\n",
    "    input_data=input_example,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "46e62454-7905-4101-a20e-88414fe2a9fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 4: Register the Model to Unity Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42cb7469-ae6b-467c-a778-7711c8255208",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "\n",
    "catalog = \"jack_sandom\"\n",
    "schema = \"ai_audience_segments\"\n",
    "model_name = \"ad_profile_agent\"\n",
    "UC_MODEL_NAME = f\"{catalog}.{schema}.{model_name}\"\n",
    "\n",
    "# register the model to UC\n",
    "uc_registered_model_info = mlflow.register_model(\n",
    "    model_uri=logged_agent_info.model_uri, name=UC_MODEL_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8199ba37-5b68-4f52-9f5a-d5eb1ea58b4a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 5: Deploy the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eda26b54-7daa-4dea-af80-adcd47aa78c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks import agents\n",
    "agents.deploy(UC_MODEL_NAME, uc_registered_model_info.version)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "02. Profile Agents",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
